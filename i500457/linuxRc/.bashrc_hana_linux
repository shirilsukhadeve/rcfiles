echo -e "sourcing .bashrc_hana on $HOSTNAME"
export USING_HANA_ENV=true
export HTTP_PROXY=
export HTTPS_PROXY=
export http_proxy=
export https_proxy=
export gopher_proxy=
export ftp_proxy=
export FTP_PROXY=
export ANT_OPTS=

if [[ -f $HOME/.HappyMake/etc/hminit.sh ]]; then
    echo -e "Sourcing $HOME/.HappyMake/etc/hminit.sh"
    source $HOME/.HappyMake/etc/hminit.sh
elif [[ -f $HANA_HOME/.HappyMake/etc/hminit.sh ]]; then
    echo -e "Sourcing $HANA_HOME/.HappyMake/etc/hminit.sh"
    source $HANA_HOME/.HappyMake/etc/hminit.sh
fi

export LD_LIBRARY_PATH=${HANA_HOME}/src/build/Debug/gen
export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:${HANA_HOME}/src/build/Debug/gen/tests_gmock
export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:${HANA_HOME}/src/build/Debug/gen/Py3
export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:${HANA_HOME}/src/build/Optimized/gen/tests_gmock
export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:${HANA_HOME}/src/build/Optimized/gen${LD_LIBRARY_PATH:+:$LD_LIBRARY_PATH:+:$LD_LIBRARY_PATH}

#export LD_LIBRARY_PATH=${HANA_HOME}/src2/build/Debug/gen
#export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:${HANA_HOME}/src2/build/Debug/gen/tests_gmock
#export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:${HANA_HOME}/src2/build/Debug/gen/Py3
#export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:${HANA_HOME}/src2/build/Optimized/gen/tests_gmock
#export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:${HANA_HOME}/src2/build/Optimized/gen${LD_LIBRARY_PATH:+:$LD_LIBRARY_PATH:+:$LD_LIBRARY_PATH}


export PATH=$PATH:${HOME}/dbaas
export PATH=$PATH:${HOME}/dbaas/landscape
export PATH=$PATH:${HOME}/sap/hdbclient
export JAVA_HOME=/usr/lib64/jvm/java-1.8.0-openjdk-1.8.0

setup_hadoop_hive()
{
    echo "setting up Hadoop and Hive"
    spark_setup_1=$1

    #export java home for hadoop
    export PATH=$PATH:$JAVA_HOME/bin

    hadoop_home=$(find $spark_setup_1 -maxdepth 1 -type d -name "*hadoop-*" | head -n 1)
    if [ -z "$hadoop_home"  ]; then
        echo "Hadoop not found in $spark_home"
        return 1
    fi

    export HADOOP_HOME=$hadoop_home
    #export HADOOP_OPTS="--add-modules java.activation --add-opens java.smartcardio/javax.smartcardio=ALL-UNNAMED --add-opens java.base/java.lang=ALL-UNNAMED"
    #export HADOOP_OPTS="$HADOOP_OPTS --add-opens java.base/java.net=ALL-UNNAMED --add-opens java.base/java.util=ALL-UNNAMED"
    export HADOOP_MAPRED_HOME=$HADOOP_HOME
    export HADOOP_COMMON_HOME=$HADOOP_HOME
    export HADOOP_HDFS_HOME=$HADOOP_HOME
    export YARN_HOME=$HADOOP_HOME
    export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
    export PATH=$(echo $PATH | tr ":" "\n" | grep -v "${HADOOP_HOME}/sbin" | tr "\n" ":" | sed 's/:$//')
    export PATH=$(echo $PATH | tr ":" "\n" | grep -v "${HADOOP_HOME}/bin" | tr "\n" ":" | sed 's/:$//')
    export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
    export CLASSPATH=$CLASSPATH:${HADOOP_HOME}/lib/*:.
    echo "Hadoop Home set to:$HADOOP_HOME"

    derby_home=$(find $spark_setup_1 -maxdepth 1 -type d -name "*db-derby*" | head -n 1)
    if [ -z "$derby_home"  ]; then
        echo "derby not found in $spark_home"
        return 1
    fi

    export DERBY_HOME=$derby_home
    export PATH=$(echo $PATH | tr ":" "\n" | grep -v "${DERBY_HOME}/bin" | tr "\n" ":" | sed 's/:$//')
    export PATH=$PATH:$DERBY_HOME/bin
    export CLASSPATH=$CLASSPATH:${DERBY_HOME}/lib/derby.jar:${DERBY_HOME}/lib/derbytools.jar
    export CLASSPATH=$CLASSPATH:${DERBY_HOME}/lib/*:.
    echo "Derby Home set to:$DERBY_HOME"

    #hive_home=$(find $spark_setup_1 -maxdepth 1 -type d -name "*hive*" | head -n 1)
    #if [ -z "$hive_home"  ]; then
        #echo "Hive not found in $spark_home"
        #return 1
    #fi

    #export HIVE_HOME=$hive_home
    #export PATH=$(echo $PATH | tr ":" "\n" | grep -v "${HIVE_HOME}/bin" | tr "\n" ":" | sed 's/:$//')
    #export PATH=$PATH:$HIVE_HOME/bin
    #export CLASSPATH=$CLASSPATH:${HIVE_HOME}/lib/*:.
    #echo "Hive Home set to:$HIVE_HOME"

}
setup_spark()
{
    # Remove existing spark_home from PATH if it exists
    export PATH=$(echo $PATH | tr ":" "\n" | grep -v "${spark_setup}/spark" | tr "\n" ":" | sed 's/:$//')

    version=$1
    spark_setup="${HOME}/sparkSetup"
    if [ -z "$version" ]; then
        echo "Usage: spark_setup <version>"
        return 1
    fi

    spark_setup_1=$(find $spark_setup -maxdepth 1 -type d -name "*$version*" | head -n 1)

    if [ -z "$spark_setup_1"  ]; then
        echo "Spark version $version not found in $spark_setup"
        return 1
    fi

    spark_home=$(find $spark_setup_1 -maxdepth 1 -type d -name "*$version*" | sed -n '2p')
    if [ -z "$spark_home"  ]; then
        echo "Spark Binaries not found in $spark_setup_1"
        return 1
    fi

    case $version in
        2.*)
            case $version in
                2.4.8)
                    spark_config='--conf spark.sql.catalogImplementation=hive --conf spark.port.maxRetries=50 --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog --packages com.sap.hana.datalake.files:sap-hdlfs:1.2.5,io.delta:delta-core_2.11:0.6.1'
                    setup_hadoop_hive $spark_setup_1
                    ;;
                *)
                    spark_config='--conf spark.sql.catalogImplementation=in-memory --conf spark.port.maxRetries=50'
                    ;;
            esac
            ;;
        3.*)
            spark_config='--conf spark.sql.catalogImplementation=hive --conf spark.port.maxRetries=50 --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog --packages com.sap.hana.datalake.files:sap-hdlfs:1.2.5,io.delta:delta-spark_2.12:3.0.0'
            setup_hadoop_hive $spark_setup_1
            ;;
        *)
            spark_config='--conf spark.port.maxRetries=50'
            ;;
    esac

    export SPARK_HOME=$spark_home
    export SPARK_CONFIG=$spark_config
    echo "Spark Home set to:$SPARK_HOME"
    export PATH=$PATH:${SPARK_HOME}/bin
    cd ${spark_setup_1}
}

alias o_pull='git merge --no-commit --no-ff --log origin/orange'
alias o_hanadt_push='git merge --no-commit --no-ff --log origin/orange-hanadt'
alias cds='cd ${HANA_HOME}/src'
alias dbaas='cd ${HANA_HOME}/dbaas'
alias cdinstall='cd ${HANA_HOME}/src/build/Optimized/gen/__installer.HDB'
alias hdlfscli='/data/i500457/hdlfscli/IQ-17_1/bin64/hdlfscli'

#exports for data_platform
export PATH="/data/i500457/dbaas/data_platform_performance/kafka_2.13-3.4.0/bin:$PATH"
alias dpe='cd /data/i500457/dbaas/data_platform_performance'
alias kafka="cd /data/i500457/dbaas/data_platform_performance/kafka_2.13-3.4.0/bin"
alias zookeeper='/data/i500457/dbaas/data_platform_performance/kafka_2.13-3.4.0/bin/zookeeper-server-start.sh /data/i500457/dbaas/data_platform_performance/kafka_2.13-3.4.0/config/zookeeper.properties'
alias broker1='/data/i500457/dbaas/data_platform_performance/kafka_2.13-3.4.0/bin/kafka-server-start.sh /data/i500457/dbaas/data_platform_performance/kafka_2.13-3.4.0/config/server1.properties'
alias broker2='/data/i500457/dbaas/data_platform_performance/kafka_2.13-3.4.0/bin/kafka-server-start.sh /data/i500457/dbaas/data_platform_performance/kafka_2.13-3.4.0/config/server2.properties'
alias broker3='/data/i500457/dbaas/data_platform_performance/kafka_2.13-3.4.0/bin/kafka-server-start.sh /data/i500457/dbaas/data_platform_performance/kafka_2.13-3.4.0/config/server3.properties'

code_hana()
{
    export HANA_H="${HANA_HOME}/src"
    export PY_H="${HANA_HOME}/src"
    ctag_dir=$HANA_HOME/cscope/`git symbolic-ref --short HEAD`/
    echo $ctag_dir
    test -d "${ctag_dir}" && rm -rf "${ctag_dir}"
    mkcd "${ctag_dir}"
    test -e cscope.out && rm cscope.out
    find $HANA_H  -name '*.h' -o -name '*.hpp' -o -name '*.cc' -o -name '*.c' -o -name '*.cpp' -o -name '*.cxx' -o -name '*.idl' -o -name '*.icpp' -o -name '*.y' -o -name '*.l' | grep -v "$HANA_H/.ccls-cache" > cscope.files
    find $PY_H -name '*.py' >> cscope.files
    cscope -b
    export CSCOPE_DB="${ctag_dir}/cscope.out"
    cd -
}
